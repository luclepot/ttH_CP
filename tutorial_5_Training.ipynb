{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import logging\n",
    "\n",
    "from madminer.core import MadMiner\n",
    "from madminer.plotting import plot_2d_morphing_basis\n",
    "from madminer.delphes import DelphesProcessor\n",
    "from madminer.sampling import combine_and_shuffle\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import constant_benchmark_theta, multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train neural networks to estimate likelihood ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to build the neural network that estimates the likelihood ratio. The central object for this is the `madminer.ml.MLForge` class. It defines functions that train, save, load, and evaluate the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge = MLForge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the RASCAL method described in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020). Other implemented methods include CARL, CASCAL, and ROLR described in the same publications. There is also SCANDAL introduced in [\"Mining gold from implicit models to improve likelihood-free inference\"](https://arxiv.org/abs/1805.12244) as well as ALICE and ALICES which are introduced in [\"Likelihood-free inference with an improved cross-entropy estimator\"](https://arxiv.org/abs/1808.00973).\n",
    "\n",
    "Most of these methods exist both in a \"single parameterized\" version, in which only the dependence of the likelihood ratio on the numerator is modelled, and a \"doubly parameterized\" version, in which both the dependence on the numerator and denominator parameters is modelled. For the single parameterized version, use `method='rascal'`, `method='alice'`, and so on. For the double parameterized version, use `method='rascal2'`, `method='alice2'`, etc. Note that for the doubly parameterized estimators you have to provide `theta1_filename`, and in the case of RASCAL and ALICE also `t_xz1_filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:22 madminer.ml          INFO    Starting training\n",
      "16:22 madminer.ml          INFO      Method:                 alice\n",
      "16:22 madminer.ml          INFO      Training data:          x at data/samples/x_train1.npy\n",
      "16:22 madminer.ml          INFO                              theta0 at data/samples/theta0_train1.npy\n",
      "16:22 madminer.ml          INFO                              y at data/samples/y_train1.npy\n",
      "16:22 madminer.ml          INFO                              r_xz at data/samples/r_xz_train1.npy\n",
      "16:22 madminer.ml          INFO                              t_xz (theta0) at data/samples/t_xz_train1.npy\n",
      "16:22 madminer.ml          INFO      Features:               all\n",
      "16:22 madminer.ml          INFO      Method:                 alice\n",
      "16:22 madminer.ml          INFO      Hidden layers:          (20, 20, 20)\n",
      "16:22 madminer.ml          INFO      Activation function:    tanh\n",
      "16:22 madminer.ml          INFO      Batch size:             512\n",
      "16:22 madminer.ml          INFO      Trainer:                amsgrad\n",
      "16:22 madminer.ml          INFO      Epochs:                 20\n",
      "16:22 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:22 madminer.ml          INFO      Validation split:       0.3\n",
      "16:22 madminer.ml          INFO      Early stopping:         True\n",
      "16:22 madminer.ml          INFO      Scale inputs:           True\n",
      "16:22 madminer.ml          INFO      Shuffle labels          False\n",
      "16:22 madminer.ml          INFO      Regularization:         None\n",
      "16:22 madminer.ml          INFO      Samples:                all\n",
      "16:22 madminer.ml          INFO    Loading training data\n",
      "16:22 madminer.ml          INFO    Found 100000 samples with 1 parameters and 2 observables\n",
      "16:22 madminer.ml          INFO    Rescaling inputs\n",
      "16:22 madminer.ml          INFO    Creating model for method alice\n",
      "16:22 madminer.ml          INFO    Training model\n",
      "16:22 madminer.utils.ml.ra INFO      Epoch 02: train loss 0.6335 (improved_xe: 0.6335)\n",
      "16:22 madminer.utils.ml.ra INFO                val. loss  0.6387 (improved_xe: 0.6277) (*)\n",
      "16:22 madminer.utils.ml.ra INFO      Epoch 04: train loss 0.6199 (improved_xe: 0.6199)\n",
      "16:22 madminer.utils.ml.ra INFO                val. loss  0.6307 (improved_xe: 0.6200) (*)\n",
      "16:22 madminer.utils.ml.ra INFO      Epoch 06: train loss 0.6178 (improved_xe: 0.6178)\n",
      "16:22 madminer.utils.ml.ra INFO                val. loss  0.6295 (improved_xe: 0.6188) (*)\n",
      "16:22 madminer.utils.ml.ra INFO      Epoch 08: train loss 0.6172 (improved_xe: 0.6172)\n",
      "16:22 madminer.utils.ml.ra INFO                val. loss  0.6287 (improved_xe: 0.6180) (*)\n",
      "16:22 madminer.utils.ml.ra INFO      Epoch 10: train loss 0.6168 (improved_xe: 0.6168)\n",
      "16:22 madminer.utils.ml.ra INFO                val. loss  0.6286 (improved_xe: 0.6179) (*)\n",
      "16:22 madminer.utils.ml.ra INFO      Epoch 12: train loss 0.6167 (improved_xe: 0.6167)\n",
      "16:22 madminer.utils.ml.ra INFO                val. loss  0.6285 (improved_xe: 0.6178)\n",
      "16:23 madminer.utils.ml.ra INFO      Epoch 14: train loss 0.6165 (improved_xe: 0.6165)\n",
      "16:23 madminer.utils.ml.ra INFO                val. loss  0.6281 (improved_xe: 0.6174) (*)\n",
      "16:23 madminer.utils.ml.ra INFO      Epoch 16: train loss 0.6164 (improved_xe: 0.6164)\n",
      "16:23 madminer.utils.ml.ra INFO                val. loss  0.6282 (improved_xe: 0.6175)\n",
      "16:23 madminer.utils.ml.ra INFO      Epoch 18: train loss 0.6163 (improved_xe: 0.6163)\n",
      "16:23 madminer.utils.ml.ra INFO                val. loss  0.6281 (improved_xe: 0.6175)\n",
      "16:23 madminer.utils.ml.ra INFO      Epoch 20: train loss 0.6162 (improved_xe: 0.6162)\n",
      "16:23 madminer.utils.ml.ra INFO                val. loss  0.6281 (improved_xe: 0.6174) (*)\n",
      "16:23 madminer.utils.ml.ra INFO    Early stopping did not improve performance\n",
      "16:23 madminer.utils.ml.ra INFO    Finished training\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='alice',\n",
    "    theta0_filename='data/samples/theta0_train1.npy',\n",
    "    x_filename='data/samples/x_train1.npy',\n",
    "    y_filename='data/samples/y_train1.npy',\n",
    "    r_xz_filename='data/samples/r_xz_train1.npy',\n",
    "    t_xz0_filename='data/samples/t_xz_train1.npy',\n",
    "    n_hidden=(20,20,20),\n",
    "#    alpha=5.,\n",
    "#    initial_lr=0.1,\n",
    "    n_epochs=20,\n",
    "    validation_split=0.3,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "forge.save('models/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue with the validation of the trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
